\documentclass[a4paper,12pt]{article}
\usepackage[T2A]{fontenc}
\usepackage[bulgarian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}
\usepackage[pdfborder={0 0 0}]{hyperref}
\begin{document}
  \title{Създаване на Web-базирана система за подготовка на ученици за състезания по информатика}
  \author{Валентин Михов}
  \date{Август 2010}
  
  \maketitle
  
  \begin{abstract}
    
    В настоящата дипломна работа ще опиша опита ми в създаването на Web система за обучение на ученици по информатика. Ще опиша как за кратко време успях да изградя ядрото на системата, за имплементацията на което обикновенно отнема много време, като така успях да се концентрирам върху важните части на системата, които се ползват от учениците и ръководителите. Освен това ще споделя опита, който натрупахме заедно с още няколко колеги, докато използвахме тази система. Какво направихме добре и какво видяхме, че не работи при подготовката на състезатели по информатика.
    \end{abstract}

  \tableofcontents
  
  \section{Въведение}
  
    Това не е първата или последната разработка на автомаизирана система за оценка на задачи по информатика. Има много разработки по темата и не малко сайтове, които предлагат изключително добре направени системи, със изключително голямо количество задачи в тях. Така например Sphere Online Judge съдържа над 5000 задачи и 40 различни езика за програмиране \cite{online_judge_list}. Въпросът е защо решихме да направим още една такава и кому това е нужно? В тази глава ще разкажа, защо България има дългогодишни традиции в състезанията по информатика и как възникна идеята за Maycamp.
  \subsection{История на IOI}
  \subsection{Българският отбор на IOI}
  \subsection{Идеята за Maycamp}
  
  Нека първо да разясния кой сме ние и как започна всичко. Всичко започна в един обед в София, когато бяхме заедно със моя колега Слави Маринов в един ресторант. Разговаряхме за това как образованието в България е доста счупено и има нужда от нещо, което да го оправи. Слави имаше идеята да направим промяна в подготовката по информатика: нещо в което имаме познания и опит. Останалото ще дойде с времето. Това беше началото. Една проста идея: да подобрим подготовката по информатика в България.
  
  След известно време вече бяхме събрали доста съмишленици, най-вече сред бившите състезатели по информатика, които са останали в България. Събрахме се не малко хора: Антон Димитров, Тодор Петров, Орлин Тенчев, Слави Маринов, Даниел Славов, Кремена Роячка, Слави Маринов, Пламена Александрова и аз.
  
  Първоначалната идея беше да се направи сайт, на който да се публикуват видео лекции. Лекциите са организирани на нива и за да можеш да преминеш на следващото ниво трябва да решиш задачите от текущата лекция, като така показваш, че си разбрал материала. Освен това всеки участник има ментор, който му помага когато има трудности.
  
  След едногодишна работа се оказа, че този формат не е толкова ефективен, колкото на нас ни се иска и е труден за поддръжка от хора, които се занимават само през свободното си време с това. Всички хора в проекта работихме на пълен работен ден и трябваше да вместваме работата по проекта в графика си. Така на годишната сбирка на екипа, след като разгледахме резултатите от анкетата, която проведохме сред учаснитиците, решихме да сменим формата на подготовката. Почти всички участници бяха посочили, че ако правим тренировъчни състезания това ще подобри новото им много повече. Така решихме да направим система, на която да даваме състезания.
  
  Системата не беше единственото нещо, което решихме да създадем. Освен редовните състезания и възможността за практика на стари задачи, решихме на сайта да има и анализи на решенията, както и статии на различни теми, като например стратегии за решаване на задачи на състезание.
  
  Беше решено, че ще направим 12 състезания през годината, като всяко състезание ще има 3 дивизии: бронзова, сребърна и златна, като разликата ще е в нивото на трудност. Идея, която взаимствахме от USACO \cite{usaco_website}. След всяко състезание ще се публикуват анализи на задачите и всяка задача ще може да се практикува, като ще има класация по практика. Освен това задачите ще се оценяват на принципа на ученическите състезания, като на една задача можеш да имаш между 0 и 100 точки според това за какъв процент от тестовете програмата ти работи.
  
  \subsubsection{Защо още една система}
  
  Един от най-важните въпроси и е защо решихме да правим изцяло нова система, след като има толкова много готови. Разбира се ние мислихме по този въпрос и като крайно прагматични хора разгледахме алтернативите. Ето и причините, които ни накараха да поемем по този път:
  
  \begin{enumerate}
    \item Искахме всички материали да са на български език - след като направихме проучване сред състезателите, се оказа че по-малките сстезатели имат проблем с подготовката от сайтове на английски език. Тъй като според нас малките състезатели са много важни (дори може би по-важни от големите), решихме че това е много важен аргумент, който трябва да имем предвид.
    \item Искаме задачите да се оценяват на принципа на ученическите състезания. Оказва се че повечето системи за подготовка се основават на ACM правилата, които гласят че една задача е или решена за 100 точки или не е решена, т.е. 0 точки. Това не беше допустимо за нас, тъй като на едно ученическо състезание е от много голямо значение всеки състезател да може да извлече всяка една точка. Това, че не може да измисли решението за 100 точки не значи, че трябва да се откаже от дадена задача. Това според нас е умение, което трябва да се тренира в участниците.
    \item Искаме да даваме лесни задачи в бронзовата дивизия. Спред нас това е жизнено важно за привличането на нови състезатели в малките възрастови групи. Повечето системи за подготовка наблягат на сериозните състезания, което кара малките и нови състезатели да се отчайват от трудността на задачите. Нашата цел е точно обратна - да накраме тези по-малки и неопитни деца да усетят тръпката на състезанието и победата над съучениците им.
    \item Възможността ние да контролираме всеки един аспект от системата. Свободата, която ще имаме ако системата е наша е много важна за да можем да вървим напред. Разбира се това идва на цена: повече време отделено за разработка и нуждата да поддържаме сами всичко.
  \end{enumerate}
  
  Разбира се ние сме наясно, че няма вечни неща на този свят и е възможно в близко бъдеще да се появи нова система правена от други хора, която да засенчи нашата. Затова и целта беше да изградим това начинание с възможно най-малко усилия, без да се задълбаваме в техничеки подробности. В следващите глави ще опиша какви решения направихме по време на имеплементацията и как всъщност с доста малко усилия изградихме всичко.
  
  \section{Изграждане на системата}
  В следващата секция ще проследя етапите през, които премина изграждането на
  Арената. Ще се опитам да опиша какво беше направено добре и какво не.
  \subsection{Изграждане на ядрото за пускане на код}
  Във всяка една система за провеждане на систезания по програмиране има ядро,
  което се занимава със пускане на решенията на участниците в контролирана среда
  и оценка на решенията срещу дадено множестно от тестове.

  Обикновенно ядрото на системата включва в себе си модул, който умее да пуска код в контролирана среда - така нареченият sandbox. Идеята на този модул, е че кода който състезателите пращат обикновенно не е коректен и пускането му може да доведе до нестабилност във системата. Освен това е нужен за да се продотврати опити за хакване на системата или за измами, като се пращат решения, които по заобиколен и нечестен начин успяват да работят за 100 точки.
  
  Има много публикации за това как може да бъде направен един подобен модул. Това е доста сложна тема и един такъв модул може да е изключително сложен. Може би това е най-трудната част в имплементирането на една подобна система.
  
  Единият от начините е да се направи модул към ядрото на Linux, с който да се следи какво прави изпълняваният код. За повече информация можете да видите \cite{linux_sec_module_sandbox}, където е анализирано подобно решение. Оказва се обаче, че подобна имплементация има доста проблеми, като например често сменящи се интерфейси за модулите за сигурност на Linux, не добра документация и като цяло не добра поддръжка на този вид модули от екипа разработващ ядрото на Linux. Освен това е добре да се отбележи, че подобен модул няма да е никак тривиален за имплементация, тъй като изисква дълбоки познания за ядрото в Linux и работа с недокументирани интерфейси.
  
  Другият начин за справяне с този проблем е с прихващане на системните извиквания. Подобно решение е ползвано и в системата Moe и повече за него може да бъде прочетено тук \cite{perspectives_grading_systems}. Накратко идеята е да се ползва така нареченият ptrace интерфейс в Linux, с който кода на състезателя за пуска в контролирана среда и всяко едно системно извикване от кода се прихваща и проверява от контролиращият модул. Ако контролираният код направи системно извикване, което не е позволено, то той се прекратява и се смята, че участника се опитва да направи нещо против правилата.
  
  Това е може би най-широко използваният в момента модел, като ситемите на IOI използват точно него. За нещастие и той има някой слаби страни: може да забави доста тестването на решенията \cite{performance_reactive_tasks}, а и не е никак тривиален за имплементация.
  
  Трябва да се отбележи, че подобни модули вече съществуват и дори могат да се ползват на готово, както например беше направено на международната олимпиада в България 2009 \cite{validating_security}. Модулът, който се ползва на тази олимпиада е от системата Moe и вече е ползван на няколко големи олимпиади и показва стабилни резултати. За съжаление, когато аз започнах да разработват системата за Maycamp, не знаех за съществуването на този модул, а нямах никакво време за да имплементирам сложни решения. Така стигнах до решение, което е доста просто като логика, много кратко и след 11000 тествани решения и над 10 проведени състезания е показало незначителни проблеми, за които ще напиша по-нататък.
  
  Решението, което предлагам предоставя защити срещу няколко основни типове атаки и мисля, че до голяма степен елиминира 95\% от възможните хакове, които биха могли да бъдат пробвани.
  
  \subsubsection{Импементация на модула за контролирано пускане на код}
  Когато започнах изграждането на модула, първото нещо което направих беше да идентифицирам основните ситуации, които искам модулът да засича. Това са:
  
  \begin{enumerate}
    \item Решения, които зациклят в безкраен цикъл
    \item Решения, които заспиват в системно извикване (например getc())
    \item Решения, които се опитват да заделят повече памет от позволеното
    \item Безкрайна рекурсия
    \item Fork-бомба
    \item Опити за установяване на мрежова връзка
  \end{enumerate}
  
  Въз основа на тези основни проблеми, направих проста система за тестване, която симулираше всяка от тези ситуации и проверяваше дали изграденият модул се справяше с тях. Самият модул представява скрипт, който приема като аргументи параметрите, в които решението трябва да се изпълни и като прикючи, в статуса му има код, който описва дали това се е случило. Ако по време на изпълнението е визникнала извънредна ситуация, статусът описва какво точно е станало. Ето и псевдо-код на скрипта:
  
  \begin{lstlisting}
    fork the current process
    in the child:
      set limit of the number of child processes
      set the priority of the current process to the lowest
      
      execute the program to be tested
    
    in the parent:
      while the child process is running:
       check the running time of the child process
       check the used memory of the child process
       check the running state of the child process
       
       if any of these are out of expected, kill the process
       
      process the exit status of the child process
  \end{lstlisting}
  
  Както виждате първото нещо, което прави той е да клонира текущият процес и да заложи накой от параметрите на новият процес. Задаването на брой позволени процеси става посредством setrlimit. След проведени тестове се оказа, че това работи стабилно под Linux и предотвратява атаките със fork-бомби, които са може би едни от най-лесните и най-опасните за подобна система.
  
  След това се сваля приоритета на новият процес до минимума, като така се гарантира, че нашият скрипт, които ще следи решението ще се изпълнява достатъчно често за да може своевременно да засече всякакви нередности.
  
  Накрая новият процес пуска кода на програмата за тестване, която напълно заменя новият процес.
  
  През това време в процеса-баща вървят постоянни проверки за състоянито на процеса-дете. На много малки интервали от време се проверява във proc файловата система, колко точно време и памет е използвана, както и състоянието на самият процес. Ако по време на една такава проверка се види, че процесът е заспал, то значи той прави нещо нередно. Нито едно от системните извиквания, които са позволени на състезателите не могат да накарат програмата да е в състояние sleep. Това означава, че е направено системно извикване, което чака някакви данни да се получат или за някакво друго събитие. Задачите, които се тестват на тази система, винаги четат от стандартният вход и пишат на него и това са едиствените системни операции, които им са позволени. Освен това винаги когато едно такова решение бива извикано, входните данни са подадени на стандартният вход, т.е. не се очаква решението да заспи в чакане на входни данни.
  
  Така чрез тези няколко изключително прости механизми се гарантира прихващането на изброените по-горе сценарии.
  
  % ********************************* Архитектура ******************************
  \subsection{Архитектура на системата}
  При изграждането на Арената, реших че е крайно неприемливо цялата система да е
  на една физическа машина. Тъй като пускането на чужд код е една крайно несигурна операция, дори и в конролирана среда, възможността при евентуална атака цялата система да бъде компрометирана ми се струваше крайно неприемлива. Затова и решението беше решенията да се оценяват на машина, която е изцяло независима от системата, която поддържа Web интерфейса към системата. Така ако има атака, чрез пращане на опасен код, сайта ще продължи да работи, а машината, която тества ще бъде афектирана - Фигура~\ref{architecture}.
  
  \begin{figure}
    \begin{center}
      \includegraphics[width=0.8\textwidth]{architecture.png}
    \end{center}
    \caption{Архитектура на системата}
    \label{architecture}
  \end{figure}
  
  Това е доста прост начин за изолация и не е никак нов при изграждането на подобни системи. Тъй като при нас все още данните са сравнително малко решихме да обединим MySQL машината с Web машината, като така намалим разходите си за инфраструктура. Така цялата система се състои от 2 машини. Едната се хоства в dreamhost.com, сериозен доставчик на подобни услуги, а другата се намира в България, на сървър, които ние закупихме за разнообразни нужди. Така израдената архитектура има своите определени преимущества, но има и някой недостатъци, с които трябваше да се преборим.
  
  Първият проблем, е скоростта на комуникация между отделните машини. В нашият случай Web частта се намира на чуждестранни сървъри, където сме сигурни, че машината ще работи 99\% от времето и ще е свързана постоянно към Интернет. От друга страна машината, която оценява решенията се намира в България. За всяко едно оценяване на решение трябва да се осъществи комуникация между двете машини, като скороста между тях е многократно по-малка от скоростта ако те бяха в една локална мрежа. Това прави нужно да се минимизира комуникацията между тях. Затова решихме двете машина да обенят единствено сорс кодовете на решенията, резултатите от тестването и логовете от тестването на решението.
  
  Това решение се оказа добро, но малко след пускането на системата се оказа, че има проблем в него. Проблема, беше не толкова в архитектурата, колкото в конкретната имплементация. Оказа се, че ако някое решение генерира много голям вход, лога на тестването става много голям, тъй като той съдържа разликата между правилният отговор и отговора на състезателя. Така ако разликата е много голяма комуникацията между Web частта и Grader-а се задръства. Това наложи да сложим ограничения на размера на показваната разлика в лога, което реши проблема.
  
  Вторият проблем е как да се пазят тестовете на задачите. Те могат да са доста големи и както видяхме в предната точка не е приемливо много данни да се прехвълят при всяко тестване. Така решихме да пазим две копия на тестовете. Едно на Web сървъра и едно на тестващият сървър. Въпросът е как синхронзираме тези две копия?
  
  Първо за да можем да сме сигурни, че във всеки един момент тестваме с най-новите тестове, преди всяко тестване се проверява поле в базата, което пази последната дата на промяна на тестовете. Ако тази дата е по-нова от датата на тестовете, които има на Grader-а в момента, тестовете се обновяват. За да работи тази схема коректное е нужно двата сървъра да имат синхронизирано време. За целата за използва NTP улугата, която синхронизира часовниците на сървърите с точното време взето от световни сървъри предназначени за това.
  
  Първоначално бях направил синхрониацията да става с изтегляне на zip файл със тестовете, но това доста бързо се оказа неподходящо. В момента тестовете на всички задачи заемат 945Mb некомпресирани, но дори и компресирани това е доста голямо количество данни за пренос между двата сървъра. За целата направих решение използващо rsync, което копира единствено променените файлове между двата сървъра. Това се оказа прекрасно решение! Авторите на задачи могат в рамките на секунди да качат нова задача и да я тестват. Времето за синхронизация на тестовете е на практика незабележима.
  
  \subsection{Изграждане потребителският интерфейс}
  \subsection{Формат на състезанията и задачите}
  \subsection{Описание на системата за рейтинги}
  
  
  \subsubsection{Математически модел}
  В тази секция ще разясня как точно работи модела на рейтингите на арената. Както вече казах, решихме да използваме системата за рейтинги на TopCoder \cite{topcoder_ratings}. Въпреки, че математическият модел е описан като формули на сайта на TopCoder, каква е идеята зад самият модел не е толкова очевидно.

  В тази секция ще се опитам да разясня как работи този модел, а в следващата секция ще видим защо всъщност е добър за нашите цели.
  
  Модела е статистически и приема, че при едно състезание разпределението на точките е нормално. Това значи, че хората с най-малко точки са малко, после тези със около среден брой точки най-много и тези които са с най-много точки също са малко. Виж Фигура~\ref{normal_distribution} за нагледен пример.
  
  \begin{figure}
    \begin{center}
      \includegraphics[width=0.8\textwidth]{normal_distribution.png}
    \end{center}
    \caption{Нормално разпределение}
    \label{normal_distribution}
  \end{figure}
  
  Системата за оценяване пази 2 стойности за всеки състезател: текущ рейтинг и променливост. Рейтинга е оценка на състезателя спрямо останалите състезатели, а променливостта оценява до колко представянията на този учасник са постоянни, т.е. ако състезателя винаги е на 1 място неговата променливост ще е малка, но ако понякога е на 1 и понякога на последно място, тогава променливостта му ще е голяма. Можем да си мислим и за рейтингите като случайни променливи, а променливостта е тяхната дисперсия. 
  
  След всяко състезание, алгоритъма преизчислява рейтингите и променливостта. Алгоритъма, който прави това взема като вход класиране на състезатели и техните рейтинги и променливост. Трябва да се отбележи, че това колко точки има всеки състезател е без значение. Важно е единствено мястото в класирането. Ако двмата състезателя имат равен брой точки, то се смята че те са на едно и също място, което е средното аритметично от местата покрити от тях, т.е. ако има трима човека на 1 място с равен брой точки, т.е. те са все едно на \((1 + 2 + 3) / 3 = 2\) място.
  
  Отначало се изчислява средният рейтинг на учасниците в състезанието:
  
  \begin{equation} \label{eq:ave_rating}
    AveRating = \frac{\sum\limits_{i=1}^{NumCoders} Rating_{i}}{NumCoders}
  \end{equation}
  
  Важно е да се отбележи, че винаги се съпоставят състезатели, които са участвали на едно и също състезание със едни и същи задачи.
  
  В тази формула \(Rating_i\) е рейтинга на \(i\)-тият състезател, а \(NumCoders\) е общият брой състезатели.
  
  Следващата стъпка е да че изчисли, конкурентността на състезанието. Това е число, което зависи от 2 компоненти. Първо от средното квадратично от променливостта на всички състезатели, и второ от това колко са различни рейтингите на всички състезатели. Колкото повече състеатели има със различни рейтинги, толкова това число е по-голямо. Ето и формулата която се ползва: 
  
  \begin{equation}
    CF = \sqrt{\\
      \frac{\sum\limits_{i=1}^{NumCoders}Volatility_i^2}{NumCoders}\\
      +\\
      \frac{\sum\limits_{i=1}^{NumCoders}(Rating_i-AveRating)^2}{NumCoders - 1}\\
    }
  \end{equation}
  
  Идеята е, че колкото по-конкурентно е едно състезание, толкова повече точки носи то.
  
  Следващата формула е може би една от най-интересните. Това което се опитва да изчисли тя е каква е вероятността един състезател да има повече точки от друг състезател, т.е. да е по-напред в класирането от него. Тъй като предполагаме, че разпределението е нормално, можем да използваме функцията за грешка на нормалното разпределение и да използваме разликата на рейтингите на двмата състезатели. Функцията за грешка има стойност 0 за 0, което ще рече, че ако имаме двмата състезателя с еднакъв, рейтинг то шанса всеки един от тях да е по-напред от дригия е 0.5, т.е. те имат равни шансове. Както виждате на Фигура~\ref{error_function} тази функция има много подходяща графика, която е много стръмна около нулата и постепенно става по-полегата, т.е. колкото е по-голяма разликата в рейтингите на двамата състезателя, толкова преимущестовото на по-силният ще е по-малко осезаемо.
  
  \begin{figure}
    \begin{center}
      \includegraphics[width=0.8\textwidth]{error_function.png}
    \end{center}
    \caption{Функция на грешката}
    \label{error_function}
  \end{figure}
  
  Тъй като рейтингите, са на практика случайни величини и когато ги изваждаме ние създаваме нова случайна величина. Затова трябва да разделим разликата на тяхната комбиниране дисперсия. Освен това имаме условието при равни рейтинги вероятността да е 0.5, което ни води към следната формула:
  
  \begin{equation}
    WP=0.5\left(erf\left(\frac{Rating1 - Rating2}{\sqrt{2(Vol1^2+Vol2^2)}}\right) + 1\right)
  \end{equation}
  
  След като имаме тази формула, можем да изчислим вероятността всеки състезател да победи всеки друг и така можем да изчислилим предполагаемото място на всеки състезател в класирането:
  
  \begin{equation}
    ERank=0.5+\sum\limits_{i=1}^{NumCoders} WP_i
  \end{equation}
  
  Където \(WP_i\) е вероятността текущият състезател да победи състезателя с номер \(i\).
  
  Следващата стъпка е да изчислим каква е вероятността състезателя да е на оцененото място. Това е лесно тъй като предполагаме, че състезателите са разпределени нормално в класирането, така че ще ползваме обратната на комулативната функция на нормалното разпределение \(\Phi\). Тук изваждаме 0.5 и делем на брой състезатели, за да можем да работим с нормалното разпределение, което работи с интервала \((0..1)\).
  
  \begin{equation}
    EPerf=-\Phi\left(\frac{ERank - 0.5}{NumCoders}\right)
  \end{equation}
  
  По аналогичен начин изчисляваме реалното представяне на състезателя като вземаме неговото реално място \(ARank\) в състезанието и изчисляваме каква е вероятността той да е на него:

  \begin{equation}
    APerf=-\Phi\left(\frac{ARank - 0.5}{NumCoders}\right)
  \end{equation}
  
  Така можем да изчислим представянето на състезателя като рейтинг. Това е просто разликата между това което очакваме и това което реално той е показал, умножено по конкурентността на състезанието, плюс старият рейтинг на състезателя:
  
  \begin{equation}
    PerfAs = OldRating + CF*(APerf - EPerf)
  \end{equation}

  Тък е много важно да отбележим какво става, когато едни състезател участва за 1 път. В този случай той няма реално рейтинг в системата и за целта му бива сложен фиктивен такъв. Всеки нов състезател започва със рейтинг 1200 и променливост 515. Освен това в началото се позволява на състезателите да си променят по-бързо рейтинга и за целта че изчислява функция на тежестта, която зависи от броя участия на състезателя:
  
  \begin{equation}
    Weight = \frac{1}{1 - \left(\frac{0.42}{timesPlayed + 1} + 0.18\right)} - 1
  \end{equation}
  
  Освен това се изчислява и максимална промяна на рейтинга:
  
  \begin{equation}
    Cap = 150 + \frac{1500}{TimesPlayed + 2}
  \end{equation}
  
  Накрая се изчислява новият рейтинг по този начин:
  
  \begin{equation}
    NewRating = \frac{Rating + Weight * PerfAs}{1 + Weight}
  \end{equation}
  Като, ако \(|NewRating - Rating| > Cap\), то NewRating се намества така, че той да не се променя повече от \(Cap\).
  
  Освен това се изчислява и новата променливост, която е приближение на дисперсията на случайна величина, като се има предвид и тежестта на състезанието:
  
  \begin{equation}
    NewVolatility=\sqrt{\\
    \frac{(NewRating - OldRating)^2}{Weight}\\
    +\\
    \frac{OldVolatility^2}{Weight + 1}\\
    }
  \end{equation}
  
  \subsubsection{Примери}
  След като разяснихме как работи модела, нека да разгледаме малко примери за да видим защо всъщност този модел е успешен и как той ни помага.
  
  \subsubsection{Прибавяне на външни състезания}
  \subsubsection{Национална ранклиста на състезателите}
  \section{Анализ на постигнатото до момента}
  \subsection{Статистика за посещаемостта на системата}
  \subsection{Анализ на анкетите пратени към участниците}
  \subsection{Статистика за популярността на състезанията}
  \section{Примери и описание на работата със системата}
  \subsection{Част за ученици}
  \subsubsection{Начална страница}
  \subsubsection{Регистрация}
  \subsubsection{Вход}
  \subsubsection{Участие на състезание}
  \subsubsection{Резултати от състезание}
  \subsubsection{Практика}
  \subsubsection{Класирания и рейтинги}
  \subsection{Част за ръководители}
  \subsubsection{Прибавяне на акаунт на ръководител}
  \subsubsection{Вход в системата}
  \subsubsection{Създаване на състезание}
  \subsubsection{Прибавяне на задачи и тестове}
  \subsubsection{Прибавяне на резултати от външно състезание}
  \subsubsection{Тестване на решения}
  
  \begin{thebibliography}{99}
    
    \bibitem{online_judge_list}
      Online judge, Wikipedia, \url{http://en.wikipedia.org/wiki/Online_judge}
    \bibitem{usaco_website}
      USA Computing Olympiad, \url{http://www.uwp.edu/sws/usaco/}
    \bibitem{topcoder_ratings}
      Algorithm Competition Rating System, TopCoder, \url{http://www.topcoder.com/wiki/display/tc/Algorithm+Competition+Rating+System}
    \bibitem{linux_sec_module_sandbox}
      Using a Linux Security Module for Contest Security, Bruce Merry, 2009
    \bibitem{perspectives_grading_systems}
      Perspectives on Grading Systems, Martin Mares, 2007
    \bibitem{performance_reactive_tasks}
      Performance Analysis of Sandboxes for Reactive Tasks, Bruce Merry, 2010
    \bibitem{validating_security}
      Validating the Security and Stability of the Grader for a Programming Contest System, Toncho Tochev, Tsvetan Bogdanov, 2010
    \bibitem{bulgaria_homeland_informatics}
      Bulgaria – The Homeland of International Competitions in Informatics for School Students, Petar S. Kenderov, 2009
    \bibitem{introduction_ioi_1989}
      Introduction to the Olympiads on Informatics, Dr. Pavel Azalov, 1989 \url{http://www.ioi2009.org/GetResource?id=238}
      
  \end{thebibliography}
\end{document}